import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter   # å¯é€‰

from urllib.parse import urlparse
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter
from pathlib import Path
import os
import shutil
import re

# Update filters to match LangGraph documentation structure
url_filter = URLPatternFilter(patterns=["*docs.llamaindex.ai*"], reverse=False)
# Keep excluding unwanted patterns
exclude_filter = URLPatternFilter(patterns=["*docs.llamaindex.ai/en/stable/*",
                                            "*wiki*",
                                            "*cloud*", "*.js", "*.css", "*.png", "*.jpg",
                                              "*.gif"], 
                                            reverse=True)


LATEST_DIR, OUTPUT_MD = Path("latest"), Path("llms.txt")

def sort_key(p: Path):
    parts = p.stem.split(".")          
    return len(parts), parts           

def combine_markdown_files():
    """Combine all markdown files into llms.txt"""
    md_files = sorted(
        (f for f in LATEST_DIR.glob("*.md") if "changelog" not in f.stem.lower()),  # skip changelog 
        key=sort_key
    )

    if not md_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° markdown æ–‡ä»¶è¿›è¡Œåˆå¹¶")
        return

    print(f"ğŸ“„ å‡†å¤‡åˆå¹¶ {len(md_files)} ä¸ªæ–‡ä»¶:")
    for f in md_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶å
        print(f"  - {f.name}")
    if len(md_files) > 5:
        print(f"  ... è¿˜æœ‰ {len(md_files) - 5} ä¸ªæ–‡ä»¶")

    combined_content = []
    for f in md_files:
        try:
            content = f.read_text(encoding="utf-8").strip()
            if content:
                combined_content.append(content)
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {f.name} æ—¶å‡ºé”™: {e}")

    if combined_content:
        OUTPUT_MD.write_text(
            "\n\n---\n\n".join(combined_content),
            encoding="utf-8"
        )
        print(f"âœ… llms.txt ä¿å­˜æˆåŠŸï¼ŒåŒ…å« {len(combined_content)} ä¸ªæ–‡æ¡£")
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆå†…å®¹å¯åˆå¹¶")

def archive_version(version_tag):
    """Archive current latest to versioned directory"""
    if not OUTPUT_MD.exists():
        print(f"âš ï¸ No llms.txt to archive for version {version_tag}")
        return
        
    archive_dir = Path(f"versions/v{version_tag}")
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # Copy current llms.txt to versioned directory
    archive_llms = archive_dir / "llms.txt"
    shutil.copy2(OUTPUT_MD, archive_llms)
    
    # Also copy individual markdown files if they exist
    if LATEST_DIR.exists():
        for file in LATEST_DIR.glob("*.md"):
            shutil.copy2(file, archive_dir / file.name)
    
    print(f"âœ… Archived version {version_tag} to {archive_dir}")

def get_current_version():
    """Get current version from LAST_VERSION file"""
    version_file = Path("LAST_VERSION")
    if version_file.exists():
        return version_file.read_text().strip()
    return None

def update_version(version_tag):
    """Update LAST_VERSION file with new version"""
    version_file = Path("LAST_VERSION")
    version_file.write_text(version_tag)
    print(f"âœ… Updated version to {version_tag}")

code_block = re.compile(r'```[^\n]*\n(.*?)```', re.DOTALL)
def strip_numeric(md_text):
    return code_block.sub(
        lambda match: "" if re.fullmatch(r'[\d\s]*', match.group(1)) else match.group(0),
        md_text
    )

async def main():
    print("ğŸš€ å¼€å§‹çˆ¬å– LlamaIndex æ–‡æ¡£...")
    
    # Create directories
    LATEST_DIR.mkdir(exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºç›®å½•: {LATEST_DIR}")
    
    # 1ï¸âƒ£ è®© Markdown ç”Ÿæˆå™¨æŠŠé“¾æ¥éƒ½ä¸¢æ‰
    md_gen = DefaultMarkdownGenerator(
        options={
            "ignore_links": True,        # æŠŠ [A](B) â†’ A
            "skip_internal_links": True  # å¹²æ‰ #é”šç‚¹
        },
        # 2ï¸âƒ£ æƒ³å†å»ç‚¹é¡µé¢æ‚è´¨å°±æŒ‚ä¸ªè¿‡æ»¤å™¨
        content_filter=PruningContentFilter(threshold=0.55)
    )

    # 3ï¸âƒ£ åªæˆª MkDocsâ€“Material çœŸæ­£çš„æ­£æ–‡å®¹å™¨
    cfg = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(
            max_depth=4, 
            filter_chain=FilterChain([url_filter, exclude_filter]),
            include_external=False,
            # max_pages=80,  # é™åˆ¶é¡µé¢æ•°é‡ç”¨äºæµ‹è¯•
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=True,
        target_elements=["article.md-content__inner.md-typeset"],  # è§†ç‰ˆæœ¬è‡ªè¡Œè°ƒæ•´
        # excluded_tags=["nav", "header", "footer", "form"],    # å†ä¿é™©
        word_count_threshold=20,
        markdown_generator=md_gen,
    )
    
    try:
        from crawl4ai.async_dispatcher import SemaphoreDispatcher  # 1ï¸âƒ£ å¼•å…¥ dispatcher
        
        dispatcher = SemaphoreDispatcher(max_session_permit=20)
        print("ğŸ•·ï¸ åˆå§‹åŒ–çˆ¬è™«...")
        
        async with AsyncWebCrawler() as crawler:
            from urllib.parse import urlparse, unquote

            print("ğŸ“¡ å¼€å§‹çˆ¬å– https://docs.llamaindex.ai/en/latest/ ...")
            results = await crawler.arun("https://docs.llamaindex.ai/en/latest/", config=cfg, dispatcher=dispatcher)
            
            print(f"ğŸ“‹ è·å¾— {len(results)} ä¸ªçˆ¬å–ç»“æœ")
            saved_count = 0
            
            for i, result in enumerate(results):
                parsed = urlparse(result.url)

                path = parsed.path.strip("/")
                safe_slug = path.replace("/", ".") if path else "index"
                safe_slug = unquote(safe_slug)

                filename = f"{safe_slug}.md"
                filepath = LATEST_DIR / filename

                # å»é™¤404å’Œç©ºå†…å®¹
                if not result.markdown or len(result.markdown) < 10:
                    print(f"â­ï¸ è·³è¿‡ç©ºå†…å®¹: {result.url}")
                    continue
                if result.markdown.strip() == "# 404 - Not found":
                    print(f"â­ï¸ è·³è¿‡404é¡µé¢: {result.url}")
                    continue

                try:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(strip_numeric(result.markdown) 
                                if result.markdown is not None 
                                else "")
                    print(f"ğŸ’¾ ä¿å­˜: {filename}")
                    saved_count += 1
                except Exception as e:
                    print(f"âŒ ä¿å­˜æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œä¿å­˜äº† {saved_count} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Combine files after crawling
    print("ğŸ”„ åˆå¹¶æ–‡ä»¶...")
    combine_markdown_files()
    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())